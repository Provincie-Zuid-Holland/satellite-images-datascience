{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from nso_ds_classes.nso_ds_normalize_scaler import scaler_class_all\n",
    "from annotations.data_preparation import extract_dataframe_pixels_values_from_tif_and_polygons\n",
    "from annotations.utils import get_scaler_filepath\n",
    "from annotations.data_loader import load_annotations_polygons\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Polygon Annotations to Pixel Annotation Parquet files\n",
    "\n",
    "This script is intended to transform given polygon annotations in geojson (made in i.e. QGis) into pixel level annotations, with scaled band values. The pixel level annotations are written to parquet files.\n",
    "Change the variables below to match the situation on your device.\n",
    "Note that these transformations are quite quickly very memory intensive.\n",
    "\n",
    "Date: 2024-01-11 \\\n",
    "Author: Pieter Kouyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "location = \"Voornes Duin\"\n",
    "images_folder = \"E:/Data/remote_sensing/satellite-images/\"\n",
    "regex = f\"*{location}*ndvi_height_asphalt_crop.tif\"\n",
    "annotations_folder = \"C:/Users/pzhadmin/Data/remote-sensing/annotations/\"\n",
    "annotations_polygon_filename_regex = \"annotaties_VoornesDuin_gecorrigeerd_100124_3_labels.geojson\"\n",
    "scaler_folder_path = \"../../scalers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_filepath = os.path.join(annotations_folder, f\"{location}_pixel_annotations.parquet\")\n",
    "pixel_scaled_filepath = os.path.join(annotations_folder, f\"{location}_pixel_annotations_scaled.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_polygons_gdf = load_annotations_polygons(annotations_folder, annotations_polygon_filename_regex, regex, images_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(pixel_filepath):\n",
    "    df = pd.read_parquet(pixel_filepath)\n",
    "else:\n",
    "    dfs = []\n",
    "    for tif_file in glob.glob(os.path.join(images_folder, regex)):\n",
    "        tif_file = tif_file.replace(\"\\\\\",\"/\")\n",
    "        print(tif_file)\n",
    "        name_tif_file = tif_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        with rasterio.open(tif_file) as dataset:\n",
    "            dfs += [\n",
    "                extract_dataframe_pixels_values_from_tif_and_polygons(\n",
    "                    tif_dataset=dataset, polygon_gdf=annotations_polygons_gdf,name_tif_file=name_tif_file\n",
    "                )\n",
    "            ]\n",
    "        \n",
    "    df = pd.concat(dfs)\n",
    "    df.to_parquet(pixel_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise DataFrame through Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_date in df_scaled['date'].unique():\n",
    "    print(image_date)\n",
    "    \n",
    "    a_normalize_scaler_class_all = scaler_class_all(\n",
    "        **{\n",
    "            f\"scaler_file_band{band}\": get_scaler_filepath(scaler_folder_path, image_date, location, band) for band in range(1,7)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    df_scaled[df_scaled['date'] == image_date] = a_normalize_scaler_class_all.transform(\n",
    "        df_scaled[df_scaled['date'] == image_date], col_names=[\"r\",\"g\",\"b\",\"i\",'ndvi','height']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save scaled dataframe\n",
    "df_scaled.to_parquet(pixel_scaled_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('remote_sensing_voorspelingen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cb1ec6dcd5a6ab2905b154621f89ae3ac2e76c5c8088d5b9b78b98be99de6d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
