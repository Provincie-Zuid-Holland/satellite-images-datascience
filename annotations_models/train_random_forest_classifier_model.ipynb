{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pixel Level Annotation Model\n",
    "\n",
    "### This notebook uses pixel level annotations to train a Random Forest Classifier to predict labels\n",
    "\n",
    "We assume Pixel level annotations are available, as produced by the \"../data/annotations/transform_polygon_annotations_to_pixels.ipynb\" notebook. Feature selection and grid_search for optimal parameters has been done in a separate notebook (\"Coepelduynen/make_train_model_on_annotations_coepelduynen.ipynb\") and those outcomes are taken as given in this notebook.\n",
    "\n",
    "Change the set Variables cell below as desired and then run the entire notebook to get cross_validation results as well as a final model trained on all data.\n",
    "\n",
    "Date: 2024-01-12\\\n",
    "Author: Pieter Kouyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pprint\n",
    "\n",
    "from training.train import train_imbalanced_model, cross_validation_balance_on_date\n",
    "from training.utils import get_cross_validation_results_filepath, get_model_filepath\n",
    "from training.metric_calculation import calculate_average_metrics, get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variables\n",
    "location = \"Voornes Duin\"\n",
    "satellite_constellation = \"PNEO\"\n",
    "annotated_pixels_filepath = \"C:/Users/pzhadmin/Data/remote-sensing/annotations/Voornes_Duin_PNEO_2024-01-29_pixel_annotations.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal parameters and features\n",
    "selected_features = ['r', 'g', 'b', 'n', 'e', 'd', 'ndvi','re_ndvi', 'height']\n",
    "optimal_parameters = {\n",
    "    \"n_estimators\": 10, \n",
    "    \"min_samples_split\": 5, \n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"max_features\": \"auto\", \n",
    "    \"bootstrap\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(annotated_pixels_filepath)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to give an indication of the amount of data points per image\n",
    "df['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to give an indication of the amount of data points per label\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We do cross-validation, where the folds are decided by the 'date' column. This is to avoid pixels from the same image from ending up in both the train and test datasets. We display the metrics averaged over the folds and write the results to a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(**optimal_parameters)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validation_balance_on_date(data=df, model=model, cv=5, features=selected_features, random_state=1337, sampling_type_boundary=100000, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_average_metrics(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_results_filepath = get_cross_validation_results_filepath(location=location, satellite_constellation=satellite_constellation, df=df)\n",
    "print(f\"Saving to {cross_validation_results_filepath}\")\n",
    "with open(cross_validation_results_filepath, \"wb\") as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Definitive model.\n",
    "\n",
    "Trains a Random Forest Classifier model on all data and writes it to a pickle file for later use. This is the definitive model output by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = RandomForestClassifier(**optimal_parameters)\n",
    "final_scaler = StandardScaler()\n",
    "\n",
    "train_imbalanced_model(\n",
    "    X_train=df[selected_features], \n",
    "    y_train=df[\"label\"], \n",
    "    model=final_model, \n",
    "    random_state=42, \n",
    "    sampling_type_boundary=100000,\n",
    "    scaler=final_scaler\n",
    ")\n",
    "pprint.pprint(get_metrics(y=df[\"label\"], X=df[selected_features], model=final_model, scaler=final_scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_artefact = {\n",
    "    \"model\": final_model,\n",
    "    \"scaler\": final_scaler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_filepath = get_model_filepath(location=location, satellite_constellation=satellite_constellation, df=df)\n",
    "print(f\"Saving to {final_model_filepath}\")\n",
    "with open(final_model_filepath, \"wb\") as file:\n",
    "    pickle.dump(final_artefact, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "67395a7eb8a935452fa20e0b395ee9a62415ad9a4d24c55126a2521c0eaca680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
